{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7992 images with corresponding labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Function to load images from a directory and return them as a list\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label_folder in os.listdir(folder_path):\n",
    "        label_folder_path = os.path.join(folder_path, label_folder) \n",
    "        for filename in os.listdir(label_folder_path):\n",
    "            img_path = os.path.join(label_folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(label_folder)  \n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "dataset_folder = '../dataset/images'\n",
    "images, labels = load_images_from_folder(dataset_folder)\n",
    "print(f\"Loaded {len(images)} images with corresponding labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "def save_images_and_labels(images, labels, images_file, labels_file):\n",
    "    np.save(images_file, np.array(images))\n",
    "    with open(labels_file, 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "save_images_and_labels(images, labels, 'images.npy', 'labels.pkl')\n",
    "print(\"Images and labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features array shape: (7992, 98596)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Extract SIFT features\n",
    "def extract_sift_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    \n",
    "    # Return an empty array if no descriptors are found\n",
    "    if descriptors is None:\n",
    "        return np.array([])  # Return an empty array\n",
    "    \n",
    "    return descriptors.flatten()\n",
    "\n",
    "# Extract HOG features\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale for HOG\n",
    "    features, hog_image = hog(img, orientations=orientations, \n",
    "                              pixels_per_cell=pixels_per_cell,\n",
    "                              cells_per_block=cells_per_block, \n",
    "                              transform_sqrt=True, \n",
    "                              visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Extract combined features (SIFT + HOG)\n",
    "def extract_combined_features(image):\n",
    "    sift_features = extract_sift_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    \n",
    "    # If no SIFT features, only use HOG\n",
    "    if sift_features.size == 0:\n",
    "        combined_features = hog_features\n",
    "    else:\n",
    "        combined_features = np.concatenate((sift_features, hog_features))\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Pad all feature vectors to the same length\n",
    "def pad_features(features, target_length):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) < target_length:\n",
    "            # Pad with zeros if the feature vector is shorter than target length\n",
    "            feature = np.pad(feature, (0, target_length - len(feature)), 'constant')\n",
    "        padded_features.append(feature)\n",
    "    return np.array(padded_features)\n",
    "\n",
    "# Load images (replace this with actual image loading code)\n",
    "# Assume `images` is a list of images\n",
    "# Example: images = [cv2.imread('image1.jpg'), cv2.imread('image2.jpg')]\n",
    "\n",
    "# Extract combined features from all images\n",
    "features = [extract_combined_features(img) for img in images]\n",
    "\n",
    "# Determine the maximum length of features to pad\n",
    "max_length = max([len(f) for f in features])\n",
    "\n",
    "# Pad the feature vectors to have the same length\n",
    "features_padded = pad_features(features, max_length)\n",
    "\n",
    "# Convert to a NumPy array\n",
    "features_arr = np.array(features_padded)\n",
    "\n",
    "print(f\"Features array shape: {features_arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cars       0.87      0.89      0.88       720\n",
      " motorcycles       0.79      0.81      0.80       390\n",
      "non_vehicles       0.89      0.86      0.87       489\n",
      "\n",
      "    accuracy                           0.86      1599\n",
      "   macro avg       0.85      0.85      0.85      1599\n",
      "weighted avg       0.86      0.86      0.86      1599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_arr, encoded_labels, test_size=0.2, random_state=42)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the trained model\n",
    "with open('svm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(svm_model, model_file)\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as label_file:\n",
    "    pickle.dump(label_encoder, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "# Feature extraction functions (same as before)\n",
    "def extract_sift_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    if descriptors is None:\n",
    "        return np.array([])\n",
    "    return descriptors.flatten()\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features, _ = hog(img, orientations=orientations, \n",
    "                      pixels_per_cell=pixels_per_cell,\n",
    "                      cells_per_block=cells_per_block, \n",
    "                      transform_sqrt=True, \n",
    "                      visualize=True, feature_vector=True)\n",
    "    return features\n",
    "def extract_combined_features(image, target_feature_length):\n",
    "    sift_features = extract_sift_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    combined_features = np.concatenate((sift_features, hog_features))\n",
    "    \n",
    "    # Ensure the feature vector length is fixed\n",
    "    if len(combined_features) < target_feature_length:\n",
    "        combined_features = np.pad(combined_features, (0, target_feature_length - len(combined_features)), 'constant')\n",
    "    elif len(combined_features) > target_feature_length:\n",
    "        combined_features = combined_features[:target_feature_length]\n",
    "    \n",
    "    return combined_features\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    # get the window and image sizes\n",
    "    h, w = window_size\n",
    "    image_h, image_w = image.shape[:2]\n",
    "\n",
    "    # loop over the image, taking steps of size `step_size`\n",
    "    for y in range(0, image_h, step_size):\n",
    "        for x in range(0, image_w, step_size):\n",
    "            # define the window\n",
    "            window = image[y:y + h, x:x + w]\n",
    "            # if the window is below the minimum window size, ignore it\n",
    "            if window.shape[:2] != window_size:\n",
    "                continue\n",
    "            # yield the current window\n",
    "            yield (x, y, window)\n",
    "\n",
    "def detect_vehicles_in_frame(frame, model, label_encoder, target_feature_length, window_size=(128, 128), step_size=32):\n",
    "    detected_objects = []\n",
    "    for (x, y, window) in sliding_window(frame, step_size, window_size):\n",
    "        if window.shape[0] != window_size[1] or window.shape[1] != window_size[0]:\n",
    "            continue\n",
    "        window_features = extract_combined_features(window, target_feature_length).reshape(1, -1)\n",
    "        prediction = model.predict(window_features)\n",
    "        predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
    "        print(predicted_class)\n",
    "        if predicted_class == 'cars':\n",
    "            detected_objects.append((x, y, predicted_class))\n",
    "            cv2.rectangle(frame, (x, y), (x + window_size[0], y + window_size[1]), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_class, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return frame, detected_objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, model, label_encoder, target_feature_length, window_size=(128, 128), step_size=32):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video dimensions\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create the video writer object\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        print(f\"Processing frame: {frame_idx}\")\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"End of video at frame {frame_idx}\")\n",
    "            break\n",
    "\n",
    "        # Detect vehicles in the current frame\n",
    "        frame, detected_objects = detect_vehicles_in_frame(frame, model, label_encoder, target_feature_length, window_size, step_size)\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Press 'q' to stop early\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release everything once job is finished\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")\n",
    "\n",
    "# Get the number of features the model expects\n",
    "target_feature_length = svm_model.n_features_in_\n",
    "\n",
    "# Process the video\n",
    "process_video('../dataset/sample videos/sample.mp4', 'output_video_path2.mp4', svm_model, label_encoder, target_feature_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "cars\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "cars\n",
      "non_vehicles\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "cars\n",
      "non_vehicles\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "cars\n",
      "motorcycles\n",
      "cars\n",
      "cars\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "cars\n",
      "non_vehicles\n",
      "cars\n",
      "cars\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "non_vehicles\n",
      "motorcycles\n",
      "cars\n",
      "cars\n",
      "non_vehicles\n",
      "cars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, window) \u001b[38;5;129;01min\u001b[39;00m sliding_window(resized, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, window_size\u001b[38;5;241m=\u001b[39m(w, h)):\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Extract combined features for the window\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Get the number of features the model expects\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     target_feature_length \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mn_features_in_\n\u001b[1;32m---> 57\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_combined_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_feature_length\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Predict with the pre-trained classifier\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(features)\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mextract_combined_features\u001b[1;34m(image, target_feature_length)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_combined_features\u001b[39m(image, target_feature_length):\n\u001b[0;32m     18\u001b[0m     sift_features \u001b[38;5;241m=\u001b[39m extract_sift_features(image)\n\u001b[1;32m---> 19\u001b[0m     hog_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hog_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((sift_features, hog_features))\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Ensure the feature vector length is fixed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mextract_hog_features\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_hog_features\u001b[39m(image, orientations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, pixels_per_cell\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m), cells_per_block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)):\n\u001b[0;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 14\u001b[0m     features, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpixels_per_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixels_per_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcells_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcells_per_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtransform_sqrt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\skimage\\_shared\\utils.py:438\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\skimage\\feature\\_hog.py:286\u001b[0m, in \u001b[0;36mhog\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m o, dr, dc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(orientations_arr, dr_arr, dc_arr):\n\u001b[0;32m    285\u001b[0m                 centre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([r \u001b[38;5;241m*\u001b[39m c_row \u001b[38;5;241m+\u001b[39m c_row \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, c \u001b[38;5;241m*\u001b[39m c_col \u001b[38;5;241m+\u001b[39m c_col \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 286\u001b[0m                 rr, cc \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m                 hog_image[rr, cc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m orientation_histogram[r, c, o]\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03mThe fourth stage computes normalization, which takes local groups of\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03mcells and contrast normalizes their overall responses before passing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03mGradient (HOG) descriptors.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\skimage\\draw\\draw.py:402\u001b[0m, in \u001b[0;36mline\u001b[1;34m(r0, c0, r1, c1)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mline\u001b[39m(r0, c0, r1, c1):\n\u001b[0;32m    364\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate line pixel coordinates.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pyramid(image, scale=1.5, min_size=(40, 40)):\n",
    "    yield image\n",
    "\n",
    "    # Generate pyramid levels until minimum size is reached\n",
    "    while True:\n",
    "        # Calculate the new image size based on\n",
    "        # the scale factor and resize the image\n",
    "        w = int(image.shape[1] / scale)\n",
    "        h = int(image.shape[0] / scale)\n",
    "        image = cv2.resize(image, (w, h))\n",
    "\n",
    "        # If the new level is too small, stop generating more levels\n",
    "        if image.shape[0] < min_size[1] or image.shape[1] < min_size[0]:\n",
    "            break\n",
    "\n",
    "        yield image\n",
    "def extract_combined_features(image, target_feature_length):\n",
    "    sift_features = extract_sift_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    combined_features = np.concatenate((sift_features, hog_features))\n",
    "    \n",
    "    # Ensure the feature vector length is fixed\n",
    "    if len(combined_features) < target_feature_length:\n",
    "        combined_features = np.pad(combined_features, (0, target_feature_length - len(combined_features)), 'constant')\n",
    "    elif len(combined_features) > target_feature_length:\n",
    "        combined_features = combined_features[:target_feature_length]  # Truncate if too long\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    # get the window and image sizes\n",
    "    h, w = window_size\n",
    "    image_h, image_w = image.shape[:2]\n",
    "\n",
    "    # loop over the image, taking steps of size `step_size`\n",
    "    for y in range(0, image_h, step_size):\n",
    "        for x in range(0, image_w, step_size):\n",
    "            # define the window\n",
    "            window = image[y:y + h, x:x + w]\n",
    "            # if the window is below the minimum window size, ignore it\n",
    "            if window.shape[:2] != window_size:\n",
    "                continue\n",
    "            # yield the current window\n",
    "            yield (x, y, window)\n",
    "\n",
    "\n",
    "image = cv2.imread(\"./main_video_frames/frame8.jpg\")\n",
    "\n",
    "w, h = 400, 400\n",
    "\n",
    "for resized in pyramid(image):\n",
    "    for (x, y, window) in sliding_window(resized, step_size=200, window_size=(w, h)):\n",
    "\n",
    "        # Extract combined features for the window\n",
    "        # Get the number of features the model expects\n",
    "        target_feature_length = svm_model.n_features_in_\n",
    "        features = extract_combined_features(window,target_feature_length).reshape(1, -1)\n",
    "\n",
    "        # Predict with the pre-trained classifier\n",
    "        prediction = svm_model.predict(features)\n",
    "        predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
    "        print(predicted_class)\n",
    "        if predicted_class== \"cars\":\n",
    "            clone = resized.copy()\n",
    "            cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Resize the image to be smaller for display purposes\n",
    "            small_clone = cv2.resize(clone, (clone.shape[1] // 2, clone.shape[0] // 2))\n",
    "            \n",
    "            # Show the smaller version of the window\n",
    "            cv2.imshow(\"Window\", small_clone)\n",
    "            cv2.waitKey(100)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_video(input_path, output_path, model, label_encoder, window_size=(128, 128), step_size=32):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    \n",
    "    # Try different codecs if 'DIVX' does not work\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MJPG'), 20, (width, height))\n",
    "\n",
    "    if not writer.isOpened():\n",
    "        print(\"Error: Couldn't initialize video writer.\")\n",
    "        return\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"End of video at frame {frame_idx}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Processing frame {frame_idx}\")\n",
    "        \n",
    "        # Ensure frame dimensions match the video writer's dimensions\n",
    "        if frame.shape[1] != width or frame.shape[0] != height:\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        \n",
    "        frame, detected_objects = detect_vehicles_in_frame(frame, model, label_encoder, window_size, step_size)\n",
    "        \n",
    "        print(f\"Detected objects in frame {frame_idx}: {detected_objects}\")\n",
    "        \n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '../dataset/sample videos/sample.mp4'\n",
    "output_video_path = './Prediction/vehicle_detection_output.mp4'\n",
    "\n",
    "process_video(input_video_path, output_video_path, svm_model, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
