{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7992 images with corresponding labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Function to load images from a directory and return them as a list\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label_folder in os.listdir(folder_path):\n",
    "        label_folder_path = os.path.join(folder_path, label_folder)\n",
    "        for filename in os.listdir(label_folder_path):\n",
    "            img_path = os.path.join(label_folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(label_folder)  \n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "dataset_folder = '../dataset/images'\n",
    "images, labels = load_images_from_folder(dataset_folder)\n",
    "print(f\"Loaded {len(images)} images with corresponding labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "def save_images_and_labels(images, labels, images_file, labels_file):\n",
    "    np.save(images_file, np.array(images))\n",
    "    with open(labels_file, 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "save_images_and_labels(images, labels, 'images.npy', 'labels.pkl')\n",
    "print(\"Images and labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features array shape: (7992, 98596)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Extract SIFT features\n",
    "def extract_sift_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    \n",
    "    # Return an empty array if no descriptors are found\n",
    "    if descriptors is None:\n",
    "        return np.array([])  # Return an empty array\n",
    "    \n",
    "    return descriptors.flatten()\n",
    "\n",
    "# Extract HOG features\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale for HOG\n",
    "    features, hog_image = hog(img, orientations=orientations, \n",
    "                              pixels_per_cell=pixels_per_cell,\n",
    "                              cells_per_block=cells_per_block, \n",
    "                              transform_sqrt=True, \n",
    "                              visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Extract combined features (SIFT + HOG)\n",
    "def extract_combined_features(image):\n",
    "    sift_features = extract_sift_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    \n",
    "    # If no SIFT features, only use HOG\n",
    "    if sift_features.size == 0:\n",
    "        combined_features = hog_features\n",
    "    else:\n",
    "        combined_features = np.concatenate((sift_features, hog_features))\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Pad all feature vectors to the same length\n",
    "def pad_features(features, target_length):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) < target_length:\n",
    "            # Pad with zeros if the feature vector is shorter than target length\n",
    "            feature = np.pad(feature, (0, target_length - len(feature)), 'constant')\n",
    "        padded_features.append(feature)\n",
    "    return np.array(padded_features)\n",
    "\n",
    "# Load images (replace this with actual image loading code)\n",
    "# Assume `images` is a list of images\n",
    "# Example: images = [cv2.imread('image1.jpg'), cv2.imread('image2.jpg')]\n",
    "\n",
    "# Extract combined features from all images\n",
    "features = [extract_combined_features(img) for img in images]\n",
    "\n",
    "# Determine the maximum length of features to pad\n",
    "max_length = max([len(f) for f in features])\n",
    "\n",
    "# Pad the feature vectors to have the same length\n",
    "features_padded = pad_features(features, max_length)\n",
    "\n",
    "# Convert to a NumPy array\n",
    "features_arr = np.array(features_padded)\n",
    "\n",
    "print(f\"Features array shape: {features_arr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cars       0.87      0.89      0.88       720\n",
      " motorcycles       0.79      0.81      0.80       390\n",
      "non_vehicles       0.89      0.86      0.87       489\n",
      "\n",
      "    accuracy                           0.86      1599\n",
      "   macro avg       0.85      0.85      0.85      1599\n",
      "weighted avg       0.86      0.86      0.86      1599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_arr, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open('svm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(svm_model, model_file)\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as label_file:\n",
    "    pickle.dump(label_encoder, label_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Feature extraction functions (same as before)\n",
    "def extract_sift_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    if descriptors is None:\n",
    "        return np.array([])\n",
    "    return descriptors.flatten()\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features, _ = hog(img, orientations=orientations, \n",
    "                      pixels_per_cell=pixels_per_cell,\n",
    "                      cells_per_block=cells_per_block, \n",
    "                      transform_sqrt=True, \n",
    "                      visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "def extract_combined_features(image, target_feature_length):\n",
    "    sift_features = extract_sift_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    combined_features = np.concatenate((sift_features, hog_features))\n",
    "    \n",
    "    # Ensure the feature vector length is fixed\n",
    "    if len(combined_features) < target_feature_length:\n",
    "        combined_features = np.pad(combined_features, (0, target_feature_length - len(combined_features)), 'constant')\n",
    "    elif len(combined_features) > target_feature_length:\n",
    "        combined_features = combined_features[:target_feature_length]  # Truncate if too long\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "def detect_vehicles_in_frame(frame, model, label_encoder, target_feature_length, window_size=(128, 128), step_size=32):\n",
    "    detected_objects = []\n",
    "    for (x, y, window) in sliding_window(frame, step_size, window_size):\n",
    "        if window.shape[0] != window_size[1] or window.shape[1] != window_size[0]:\n",
    "            continue\n",
    "        window_features = extract_combined_features(window, target_feature_length).reshape(1, -1)\n",
    "        prediction = model.predict(window_features)\n",
    "        predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
    "        if predicted_class == 'cars':\n",
    "            detected_objects.append((x, y, predicted_class))\n",
    "            cv2.rectangle(frame, (x, y), (x + window_size[0], y + window_size[1]), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_class, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return frame, detected_objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m target_feature_length \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mn_features_in_\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Process the video\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../dataset/sample videos/sample.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_video_path.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_label_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_feature_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(input_path, output_path, model, label_encoder, target_feature_length, window_size, step_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Detect vehicles in the current frame\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m frame, detected_objects \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_vehicles_in_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_feature_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Write the processed frame to the output video\u001b[39;00m\n\u001b[0;32m     28\u001b[0m writer\u001b[38;5;241m.\u001b[39mwrite(frame)\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mdetect_vehicles_in_frame\u001b[1;34m(frame, model, label_encoder, target_feature_length, window_size, step_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     41\u001b[0m window_features \u001b[38;5;241m=\u001b[39m extract_combined_features(window, target_feature_length)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(prediction)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcars\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\svm\\_base.py:813\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    811\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\svm\\_base.py:430\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    428\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    429\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\svm\\_base.py:449\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    445\u001b[0m         )\n\u001b[0;32m    447\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_video(input_path, output_path, model, label_encoder, target_feature_length, window_size=(128, 128), step_size=32):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video dimensions\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create the video writer object\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        print(f\"Processing frame: {frame_idx}\")\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"End of video at frame {frame_idx}\")\n",
    "            break\n",
    "\n",
    "        # Detect vehicles in the current frame\n",
    "        frame, detected_objects = detect_vehicles_in_frame(frame, model, label_encoder, target_feature_length, window_size, step_size)\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Press 'q' to stop early\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release everything once job is finished\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")\n",
    "\n",
    "# Get the number of features the model expects\n",
    "target_feature_length = svm_model.n_features_in_\n",
    "\n",
    "# Process the video\n",
    "process_video('../dataset/sample videos/sample.mp4', 'output_video_path.mp4', svm_model, label_encoder, target_feature_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 98596 features, but GaussianNB is expecting 57348 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_combined_features(window,target_feature_length)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Predict with the pre-trained classifier\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(prediction)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_class)\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\naive_bayes.py:101\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\naive_bayes.py:269\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\VS-code\\Github\\Project02\\vnev\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 98596 features, but GaussianNB is expecting 57348 features as input."
     ]
    }
   ],
   "source": [
    "with open('./saved_models/01-naive-bayes/nb_model.pkl', 'rb') as label_file:\n",
    "    model = pickle.load(label_file)\n",
    "\n",
    "\n",
    "def pyramid(image, scale=1.5, min_size=(40, 40)):\n",
    "    yield image\n",
    "\n",
    "    # Generate pyramid levels until minimum size is reached\n",
    "    while True:\n",
    "        # Calculate the new image size based on\n",
    "        # the scale factor and resize the image\n",
    "        w = int(image.shape[1] / scale)\n",
    "        h = int(image.shape[0] / scale)\n",
    "        image = cv2.resize(image, (w, h))\n",
    "\n",
    "        # If the new level is too small, stop generating more levels\n",
    "        if image.shape[0] < min_size[1] or image.shape[1] < min_size[0]:\n",
    "            break\n",
    "\n",
    "        yield image\n",
    "\n",
    "\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    # get the window and image sizes\n",
    "    h, w = window_size\n",
    "    image_h, image_w = image.shape[:2]\n",
    "\n",
    "    # loop over the image, taking steps of size `step_size`\n",
    "    for y in range(0, image_h, step_size):\n",
    "        for x in range(0, image_w, step_size):\n",
    "            # define the window\n",
    "            window = image[y:y + h, x:x + w]\n",
    "            # if the window is below the minimum window size, ignore it\n",
    "            if window.shape[:2] != window_size:\n",
    "                continue\n",
    "            # yield the current window\n",
    "            yield (x, y, window)\n",
    "\n",
    "\n",
    "image = cv2.imread(\"./main_video_frames/frame8.jpg\")\n",
    "\n",
    "w, h = 156, 156\n",
    "\n",
    "for resized in pyramid(image):\n",
    "    for (x, y, window) in sliding_window(resized, step_size=40, window_size=(w, h)):\n",
    "\n",
    "        # Extract combined features for the window\n",
    "        # Get the number of features the model expects\n",
    "        target_feature_length = svm_model.n_features_in_\n",
    "        features = extract_combined_features(window,target_feature_length).reshape(1, -1)\n",
    "\n",
    "        # Predict with the pre-trained classifier\n",
    "        prediction = model.predict(features)\n",
    "        predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
    "        print(predicted_class)\n",
    "        if predicted_class== \"cars\":\n",
    "            clone = resized.copy()\n",
    "            cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Resize the image to be smaller for display purposes\n",
    "            small_clone = cv2.resize(clone, (clone.shape[1] // 2, clone.shape[0] // 2))\n",
    "            \n",
    "            # Show the smaller version of the window\n",
    "            cv2.imshow(\"Window\", small_clone)\n",
    "            cv2.waitKey(100)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_video(input_path, output_path, model, label_encoder, window_size=(128, 128), step_size=32):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    \n",
    "    # Try different codecs if 'DIVX' does not work\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MJPG'), 20, (width, height))\n",
    "\n",
    "    if not writer.isOpened():\n",
    "        print(\"Error: Couldn't initialize video writer.\")\n",
    "        return\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"End of video at frame {frame_idx}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Processing frame {frame_idx}\")\n",
    "        \n",
    "        # Ensure frame dimensions match the video writer's dimensions\n",
    "        if frame.shape[1] != width or frame.shape[0] != height:\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        \n",
    "        frame, detected_objects = detect_vehicles_in_frame(frame, model, label_encoder, window_size, step_size)\n",
    "        \n",
    "        print(f\"Detected objects in frame {frame_idx}: {detected_objects}\")\n",
    "        \n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '../dataset/sample videos/sample.mp4'\n",
    "output_video_path = './Prediction/vehicle_detection_output.mp4'\n",
    "\n",
    "process_video(input_video_path, output_video_path, svm_model, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
